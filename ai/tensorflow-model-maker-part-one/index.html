<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building Live Detection ML Model Using Tensorflow Model Maker Part One | AI Vision</title><meta name=keywords content="AI,Machine Learning,Data,AutoML"><meta name=description content="In this blog post, we will show you how to build a live object detection machine learning model using TensorFlow Model Maker. This is the first part of a series of posts that will guide you through the process of creating and training a model using TensorFlow Model Maker. "><meta name=author content="Abdullah Al Hadrami"><link rel=canonical href=https://www.aivision.app/ai/tensorflow-model-maker-part-one/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.aivision.app/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.aivision.app/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.aivision.app/favicon-32x32.png><link rel=apple-touch-icon href=https://www.aivision.app/apple-touch-icon.png><link rel=mask-icon href=https://www.aivision.app/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Building Live Detection ML Model Using Tensorflow Model Maker Part One"><meta property="og:description" content="In this blog post, we will show you how to build a live object detection machine learning model using TensorFlow Model Maker. This is the first part of a series of posts that will guide you through the process of creating and training a model using TensorFlow Model Maker. "><meta property="og:type" content="article"><meta property="og:url" content="https://www.aivision.app/ai/tensorflow-model-maker-part-one/"><meta property="article:section" content="ai"><meta property="article:modified_time" content="2022-12-17T19:24:44+04:00"><meta property="og:site_name" content="AI Vision"><meta name=twitter:card content="summary"><meta name=twitter:title content="Building Live Detection ML Model Using Tensorflow Model Maker Part One"><meta name=twitter:description content="In this blog post, we will show you how to build a live object detection machine learning model using TensorFlow Model Maker. This is the first part of a series of posts that will guide you through the process of creating and training a model using TensorFlow Model Maker. "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"","item":"https://www.aivision.app/ai/"},{"@type":"ListItem","position":3,"name":"Building Live Detection ML Model Using Tensorflow Model Maker Part One","item":"https://www.aivision.app/ai/tensorflow-model-maker-part-one/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building Live Detection ML Model Using Tensorflow Model Maker Part One","name":"Building Live Detection ML Model Using Tensorflow Model Maker Part One","description":"In this blog post, we will show you how to build a live object detection machine learning model using TensorFlow Model Maker. This is the first part of a series of posts that will guide you through the process of creating and training a model using TensorFlow Model Maker. ","keywords":["AI","Machine Learning","Data","AutoML"],"articleBody":"The Efficientdet lite object detection model is a lightweight, high-performing model developed by Google for deployment on devices with limited resources such as smartphones and edge devices. It is compatible with a wide range of platforms and devices including Android, Linux, Windows, Chrome OS, and iPhone, thanks to the cross-platform compatibility of the Tensorflow runtime. The model can achieve an inference speed of 20 milliseconds, making it fast for low-resource devices, and it can also be run on Intel CPUs without a GPU using the “XNNPack delegate” feature. Google provides an easy-to-use API called “Tensorflow model maker” for building and training the model, simplifying the development process. In this tutorial, we will show you how to prepare a dataset, build and train an object detection model using efficientnet lite architecture and Tensorflow Lite model maker, and use the trained model to detect apples and oranges in images. We will also provide example code and a complete running notebook for Google Colab, as well as example code for running the model on edge devices and devices with Intel CPUs but no GPU, and an Android application for live inference.\nLive Demo for Android and EDGE Device Here is the live demo for Object Detection App on Android and Edge Devices.\nCreating the dataset Collect the images you want to detect.\nGo to https://www.makesense.ai/ Choose “Get Started” and then choose the images which you already collected from your computer.\nAfter uploading the images, choose “Object Detection” and then “Start Labeling”.\nyou will see a dialog box asking you to create labels , in my case i created “apple” and “orange” labels , as shown in the image below . then select start project.\nthen start labeling the images , as shown in the image below. After labeling all of the images, go to “Actions” and select “Export Annotations”.\nChoose the VOC XML format and then download the zip file.\nExtract the zip file and you will see a folder containing the XML files for each image.\ntake 20% of the images and put them in a separate folder along with the XML files for each image, this will be our test set , the rest of the images will be our training set.\nyou will end up with two folders , one for the training set and one for the test set , each folder contains the image folder and the XML folder , as shown in the image below.\nZip the dataset folder and upload them to Google Drive or github\nclick the below notebook link , which contain the running example of all code.\nLink to Colab Notebook\nif you are using Google Drive , from the left menu select “Files” and then “Mount Drive” , then click on the link and follow the instructions to mount your drive , then create the following folders by running the below cell in notebook is provided earlier. !mkdir raw_data !mkdir train_data then copy the dataset from your drive to the raw_data folder , as below code , you can find the path of the dataset from left menu then expand the drive folder and then expand the folder you uploaded the dataset to , then right click on the dataset and select “Copy Path” , then paste it in the code below , then run the cell. !cp /content/drive/MyDrive/tensorflow_lite_dataset/dataset_apple_orange.zip /content/raw_data if you are hosting the dataset on github , then you can skip the above step and run the below code to download the dataset from github , change the url to your dataset url. !wget -P /content/raw_data \u003chttps://raw.githubusercontent.com/Abdullamhd/od_efficientdet/main/dataset_apple_orange.zip\u003e then unzip the dataset to train_data folder , as shown in the below code , you can change the path of the dataset if you are using a different path. %%capture !unzip /content/raw_data/dataset_apple_orange.zip -d /content/train_data Now our dataset is ready , we will choose the model from the below table , the model is postfixed with number from zero to four , the number indicates the size of the model , the bigger the number the bigger the model size , the bigger the model size the better the accuracy , but the bigger the model size the slower the inference time , so you have to choose the model size based on your use case , i will choose the EfficientDet-Lite0 model , which is the smallest model , the model size is 4.4 MB , the inference time is 37 ms (Latency measured on Pixel 4 using 4 threads on CPU) , and the average precision is 25.69%. Model architecture Size(MB)* Latency(ms)** Average Precision*** EfficientDet-Lite0 4.4 37 25.69% EfficientDet-Lite1 5.8 49 30.55% EfficientDet-Lite2 7.2 69 33.97% EfficientDet-Lite3 11.4 116 37.70% EfficientDet-Lite4 19.9 260 41.96% Installing \u0026 Importing the required libraries let’s install and import the required libraries , shown in the below code. %%capture !sudo apt -y install libportaudio2 !pip install protobuf==3.19.4 !pip install -q --use-deprecated=legacy-resolver tflite-model-maker !pip install -q pycocotools !pip install -q opencv-python-headless==4.1.2.30 !pip uninstall -y tensorflow \u0026\u0026 pip install -q tensorflow==2.8.0 Now we will import the required libraries , shown in the below code. import numpy as np import os from tflite_model_maker.config import QuantizationConfig from tflite_model_maker.config import ExportFormat from tflite_model_maker import model_spec from tflite_model_maker import object_detector import tensorflow as tf assert tf.__version__.startswith('2') tf.get_logger().setLevel('ERROR') from absl import logging logging.set_verbosity(logging.ERROR) Choosing the model spec Now we will choose the model spec , shown in the below code , you can choose any model from the table above , i will choose the EfficientDet-Lite0 model , which is the smallest model and recommended for mobile devices. spec = model_spec.get('efficientdet_lite0') Now we will load the dataset , shown in the below code. train_data = object_detector.DataLoader.from_pascal_voc('/content/train_data/dataset/train/Images', '/content/train_data/dataset/train/Annotations', label_map={1: \"apple\",2:\"orange\"}) test_data = object_detector.DataLoader.from_pascal_voc('/content/train_data/dataset/test/Images', '/content/train_data/dataset/test/Annotations', label_map={1: \"apple\",2:\"orange\"}) Now we will train the model , shown in the below code , the default epoches is 50 , the batch size is 8 , the train_whole_model is set to True, this will train the whole model, if you set it to False , it will train only the last layer. model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=test_data) once the training is done , we will export the model , shown in the below code , the model will be exported to the current directory , then you can copy it to your google drive if you are already mounted it , or you can download it from the left menu. model.export(export_dir='.') for copying the model to your google drive , run the below code !cp /content/model.tflite /content/drive/MyDrive/tensorflow_lite_dataset Summary In the next post we will see how to deploy the model to android device , and linux machine , and how to use it in our applications , stay tuned.\n","wordCount":"1112","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"2022-12-17T19:24:44+04:00","author":{"@type":"Person","name":"Abdullah Al Hadrami"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.aivision.app/ai/tensorflow-model-maker-part-one/"},"publisher":{"@type":"Organization","name":"AI Vision","logo":{"@type":"ImageObject","url":"https://www.aivision.app/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.aivision.app accesskey=h title="AI Vision (Alt + H)">AI Vision</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.aivision.app/ai/ title=AI><span>AI</span></a></li><li><a href=https://www.aivision.app/automation/ title=Automation><span>Automation</span></a></li><li><a href=https://www.aivision.app/data/ title=Data><span>Data</span></a></li><li><a href=https://www.aivision.app/general/ title=General><span>General</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.aivision.app>Home</a>&nbsp;»&nbsp;<a href=https://www.aivision.app/ai/></a></div><h1 class=post-title>Building Live Detection ML Model Using Tensorflow Model Maker Part One</h1><div class=post-description>In this blog post, we will show you how to build a live object detection machine learning model using TensorFlow Model Maker. This is the first part of a series of posts that will guide you through the process of creating and training a model using TensorFlow Model Maker.</div><div class=post-meta>6 min&nbsp;·&nbsp;1112 words&nbsp;·&nbsp;Abdullah Al Hadrami</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#creating-the-dataset>Creating the dataset</a></li><li><a href=#installing--importing-the-required-libraries>Installing & Importing the required libraries</a></li><li><a href=#choosing-the-model-spec>Choosing the model spec</a></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div><div class=post-content><p>The Efficientdet lite object detection model is a lightweight, high-performing model developed by Google for deployment on devices with limited resources such as smartphones and edge devices. It is compatible with a wide range of platforms and devices including Android, Linux, Windows, Chrome OS, and iPhone, thanks to the cross-platform compatibility of the Tensorflow runtime. The model can achieve an inference speed of 20 milliseconds, making it fast for low-resource devices, and it can also be run on Intel CPUs without a GPU using the &ldquo;XNNPack delegate&rdquo; feature. Google provides an easy-to-use API called &ldquo;Tensorflow model maker&rdquo; for building and training the model, simplifying the development process. In this tutorial, we will show you how to prepare a dataset, build and train an object detection model using efficientnet lite architecture and Tensorflow Lite model maker, and use the trained model to detect apples and oranges in images. We will also provide example code and a complete running notebook for Google Colab, as well as example code for running the model on edge devices and devices with Intel CPUs but no GPU, and an Android application for live inference.</p><h1 id=live-demo-for-android-and-edge-device>Live Demo for Android and EDGE Device<a hidden class=anchor aria-hidden=true href=#live-demo-for-android-and-edge-device>#</a></h1><p>Here is the live demo for Object Detection App on Android and Edge Devices.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/tkFKGSHs1Ks style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h2 id=creating-the-dataset>Creating the dataset<a hidden class=anchor aria-hidden=true href=#creating-the-dataset>#</a></h2><ul><li><p>Collect the images you want to detect.</p></li><li><p>Go to <a href=https://www.makesense.ai/>https://www.makesense.ai/</a>
Choose &ldquo;Get Started&rdquo; and then choose the images which you already collected from your computer.</p></li><li><p>After uploading the images, choose &ldquo;Object Detection&rdquo; and then &ldquo;Start Labeling&rdquo;.</p></li><li><p>you will see a dialog box asking you to create labels , in my case i created &ldquo;apple&rdquo; and &ldquo;orange&rdquo; labels , as shown in the image below . then select start project.</p></li></ul><p><img loading=lazy src=create_labels.png alt="alt text" title="Create Labels"></p><ul><li>then start labeling the images , as shown in the image below.</li></ul><p><img loading=lazy src=labeling_images.png alt="alt text" title=Labeling></p><ul><li><p>After labeling all of the images, go to &ldquo;Actions&rdquo; and select &ldquo;Export Annotations&rdquo;.</p></li><li><p>Choose the VOC XML format and then download the zip file.</p></li><li><p>Extract the zip file and you will see a folder containing the XML files for each image.</p></li><li><p>take 20% of the images and put them in a separate folder along with the XML files for each image, this will be our test set , the rest of the images will be our training set.</p></li><li><p>you will end up with two folders , one for the training set and one for the test set , each folder contains the image folder and the XML folder , as shown in the image below.</p></li></ul><p><img loading=lazy src=dataset_folders_image.png alt="alt text" title=Dataset></p><ul><li><p>Zip the dataset folder and upload them to Google Drive or github</p></li><li><p>click the below notebook link , which contain the running example of all code.</p></li></ul><p><a href=https://colab.research.google.com/github/Abdullamhd/od_efficientdet/blob/main/tflite_model_maker_github_hosted.ipynb>Link to Colab Notebook</a></p><ul><li>if you are using Google Drive , from the left menu select &ldquo;Files&rdquo; and then &ldquo;Mount Drive&rdquo; , then click on the link and follow the instructions to mount your drive , then create the following folders by running the below cell in notebook is provided earlier.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>!<span style=color:#a6e22e>mkdir</span> <span style=color:#a6e22e>raw_data</span>
</span></span><span style=display:flex><span>!<span style=color:#a6e22e>mkdir</span> <span style=color:#a6e22e>train_data</span>
</span></span><span style=display:flex><span> 
</span></span></code></pre></div><ul><li>then copy the dataset from your drive to the raw_data folder , as below code , you can find the path
of the dataset from left menu then expand the drive folder and then expand the folder you uploaded the dataset to , then right click on the dataset and select &ldquo;Copy Path&rdquo; , then paste it in the code below , then run the cell.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>!<span style=color:#a6e22e>cp</span> <span style=color:#f92672>/</span><span style=color:#a6e22e>content</span><span style=color:#f92672>/</span><span style=color:#a6e22e>drive</span><span style=color:#f92672>/</span><span style=color:#a6e22e>MyDrive</span><span style=color:#f92672>/</span><span style=color:#a6e22e>tensorflow_lite_dataset</span><span style=color:#f92672>/</span><span style=color:#a6e22e>dataset_apple_orange</span>.<span style=color:#a6e22e>zip</span> <span style=color:#f92672>/</span><span style=color:#a6e22e>content</span><span style=color:#f92672>/</span><span style=color:#a6e22e>raw_data</span>
</span></span><span style=display:flex><span> 
</span></span></code></pre></div><ul><li>if you are hosting the dataset on github , then you can skip the above step and run the below code to download the dataset from github , change the url to your dataset url.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>!<span style=color:#a6e22e>wget</span> <span style=color:#f92672>-</span><span style=color:#a6e22e>P</span> <span style=color:#f92672>/</span><span style=color:#a6e22e>content</span><span style=color:#f92672>/</span><span style=color:#a6e22e>raw_data</span> &lt;<span style=color:#a6e22e>https</span>:<span style=color:#75715e>//raw.githubusercontent.com/Abdullamhd/od_efficientdet/main/dataset_apple_orange.zip&gt;
</span></span></span><span style=display:flex><span><span style=color:#75715e></span> 
</span></span></code></pre></div><ul><li>then unzip the dataset to train_data folder , as shown in the below code , you can change the path of the dataset if you are using a different path.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#f92672>%%</span><span style=color:#a6e22e>capture</span>
</span></span><span style=display:flex><span>!<span style=color:#a6e22e>unzip</span> <span style=color:#f92672>/</span><span style=color:#a6e22e>content</span><span style=color:#f92672>/</span><span style=color:#a6e22e>raw_data</span><span style=color:#f92672>/</span><span style=color:#a6e22e>dataset_apple_orange</span>.<span style=color:#a6e22e>zip</span>  <span style=color:#f92672>-</span><span style=color:#a6e22e>d</span> <span style=color:#f92672>/</span><span style=color:#a6e22e>content</span><span style=color:#f92672>/</span><span style=color:#a6e22e>train_data</span>
</span></span><span style=display:flex><span> 
</span></span></code></pre></div><ul><li>Now our dataset is ready , we will choose the model from the below table , the model is postfixed with
number from zero to four , the number indicates the size of the model , the bigger the number the bigger the model size , the bigger the model size the better the accuracy , but the bigger the model size the slower the inference time , so you have to choose the model size based on your use case , i will choose the
EfficientDet-Lite0 model , which is the smallest model , the model size is 4.4 MB , the inference time is 37 ms (<em>Latency measured on Pixel 4 using 4 threads on CPU</em>) , and the average precision is 25.69%.</li></ul><table><thead><tr><th>Model architecture</th><th>Size(MB)*</th><th>Latency(ms)**</th><th>Average Precision***</th></tr></thead><tbody><tr><td>EfficientDet-Lite0</td><td>4.4</td><td>37</td><td>25.69%</td></tr><tr><td>EfficientDet-Lite1</td><td>5.8</td><td>49</td><td>30.55%</td></tr><tr><td>EfficientDet-Lite2</td><td>7.2</td><td>69</td><td>33.97%</td></tr><tr><td>EfficientDet-Lite3</td><td>11.4</td><td>116</td><td>37.70%</td></tr><tr><td>EfficientDet-Lite4</td><td>19.9</td><td>260</td><td>41.96%</td></tr></tbody></table><h2 id=installing--importing-the-required-libraries>Installing & Importing the required libraries<a hidden class=anchor aria-hidden=true href=#installing--importing-the-required-libraries>#</a></h2><ul><li>let&rsquo;s install and import the required libraries , shown in the below code.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span> <span style=color:#f92672>%%</span><span style=color:#a6e22e>capture</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>!<span style=color:#a6e22e>sudo</span> <span style=color:#a6e22e>apt</span> <span style=color:#f92672>-</span><span style=color:#a6e22e>y</span> <span style=color:#a6e22e>install</span> <span style=color:#a6e22e>libportaudio2</span>
</span></span><span style=display:flex><span>!<span style=color:#a6e22e>pip</span> <span style=color:#a6e22e>install</span> <span style=color:#a6e22e>protobuf</span><span style=color:#f92672>==</span><span style=color:#ae81ff>3.19.4</span>
</span></span><span style=display:flex><span>!<span style=color:#a6e22e>pip</span> <span style=color:#a6e22e>install</span> <span style=color:#f92672>-</span><span style=color:#a6e22e>q</span> <span style=color:#f92672>--</span><span style=color:#a6e22e>use</span><span style=color:#f92672>-</span><span style=color:#a6e22e>deprecated</span>=<span style=color:#a6e22e>legacy</span><span style=color:#f92672>-</span><span style=color:#a6e22e>resolver</span> <span style=color:#a6e22e>tflite</span><span style=color:#f92672>-</span><span style=color:#a6e22e>model</span><span style=color:#f92672>-</span><span style=color:#a6e22e>maker</span>
</span></span><span style=display:flex><span>!<span style=color:#a6e22e>pip</span> <span style=color:#a6e22e>install</span> <span style=color:#f92672>-</span><span style=color:#a6e22e>q</span> <span style=color:#a6e22e>pycocotools</span>
</span></span><span style=display:flex><span>!<span style=color:#a6e22e>pip</span> <span style=color:#a6e22e>install</span> <span style=color:#f92672>-</span><span style=color:#a6e22e>q</span> <span style=color:#a6e22e>opencv</span><span style=color:#f92672>-</span><span style=color:#a6e22e>python</span><span style=color:#f92672>-</span><span style=color:#a6e22e>headless</span><span style=color:#f92672>==</span><span style=color:#ae81ff>4.1.2.30</span>
</span></span><span style=display:flex><span>!<span style=color:#a6e22e>pip</span> <span style=color:#a6e22e>uninstall</span> <span style=color:#f92672>-</span><span style=color:#a6e22e>y</span> <span style=color:#a6e22e>tensorflow</span> <span style=color:#f92672>&amp;&amp;</span> <span style=color:#a6e22e>pip</span> <span style=color:#a6e22e>install</span> <span style=color:#f92672>-</span><span style=color:#a6e22e>q</span> <span style=color:#a6e22e>tensorflow</span><span style=color:#f92672>==</span><span style=color:#ae81ff>2.8.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span></code></pre></div><ul><li>Now we will import the required libraries , shown in the below code.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tflite_model_maker.config <span style=color:#f92672>import</span> QuantizationConfig
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tflite_model_maker.config <span style=color:#f92672>import</span> ExportFormat
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tflite_model_maker <span style=color:#f92672>import</span> model_spec
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tflite_model_maker <span style=color:#f92672>import</span> object_detector
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> tf<span style=color:#f92672>.</span>__version__<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#39;2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tf<span style=color:#f92672>.</span>get_logger()<span style=color:#f92672>.</span>setLevel(<span style=color:#e6db74>&#39;ERROR&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> absl <span style=color:#f92672>import</span> logging
</span></span><span style=display:flex><span>logging<span style=color:#f92672>.</span>set_verbosity(logging<span style=color:#f92672>.</span>ERROR)
</span></span></code></pre></div><h2 id=choosing-the-model-spec>Choosing the model spec<a hidden class=anchor aria-hidden=true href=#choosing-the-model-spec>#</a></h2><ul><li>Now we will choose the model spec , shown in the below code , you can choose any model from the table above , i will choose the EfficientDet-Lite0 model , which is the smallest model and recommended for mobile devices.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>spec <span style=color:#f92672>=</span> model_spec<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;efficientdet_lite0&#39;</span>)
</span></span></code></pre></div><ul><li>Now we will load the dataset , shown in the below code.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_data  <span style=color:#f92672>=</span> object_detector<span style=color:#f92672>.</span>DataLoader<span style=color:#f92672>.</span>from_pascal_voc(<span style=color:#e6db74>&#39;/content/train_data/dataset/train/Images&#39;</span>, <span style=color:#e6db74>&#39;/content/train_data/dataset/train/Annotations&#39;</span>, label_map<span style=color:#f92672>=</span>{<span style=color:#ae81ff>1</span>: <span style=color:#e6db74>&#34;apple&#34;</span>,<span style=color:#ae81ff>2</span>:<span style=color:#e6db74>&#34;orange&#34;</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>test_data <span style=color:#f92672>=</span>  object_detector<span style=color:#f92672>.</span>DataLoader<span style=color:#f92672>.</span>from_pascal_voc(<span style=color:#e6db74>&#39;/content/train_data/dataset/test/Images&#39;</span>, <span style=color:#e6db74>&#39;/content/train_data/dataset/test/Annotations&#39;</span>, label_map<span style=color:#f92672>=</span>{<span style=color:#ae81ff>1</span>: <span style=color:#e6db74>&#34;apple&#34;</span>,<span style=color:#ae81ff>2</span>:<span style=color:#e6db74>&#34;orange&#34;</span>})
</span></span></code></pre></div><ul><li>Now we will train the model , shown in the below code , the default epoches is 50 , the batch size is 8 , the train_whole_model is set to True, this will train the whole model, if you set it to False , it will train only the last layer.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> object_detector<span style=color:#f92672>.</span>create(train_data, model_spec<span style=color:#f92672>=</span>spec, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>, train_whole_model<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, validation_data<span style=color:#f92672>=</span>test_data)
</span></span></code></pre></div><ul><li>once the training is done , we will export the model , shown in the below code , the model will be exported to the current directory , then you can copy it to your google drive if you are already mounted it , or you can download it from the left menu.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>.</span>export(export_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;.&#39;</span>)
</span></span></code></pre></div><ul><li>for copying the model to your google drive , run the below code<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>    !<span style=color:#a6e22e>cp</span> <span style=color:#f92672>/</span><span style=color:#a6e22e>content</span><span style=color:#f92672>/</span><span style=color:#a6e22e>model</span>.<span style=color:#a6e22e>tflite</span> <span style=color:#f92672>/</span><span style=color:#a6e22e>content</span><span style=color:#f92672>/</span><span style=color:#a6e22e>drive</span><span style=color:#f92672>/</span><span style=color:#a6e22e>MyDrive</span><span style=color:#f92672>/</span><span style=color:#a6e22e>tensorflow_lite_dataset</span>
</span></span><span style=display:flex><span> 
</span></span></code></pre></div></li></ul><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>In the next post we will see how to deploy the model to android device , and linux machine , and how to use it in our applications , stay tuned.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://www.aivision.app/ai/tensorflow-model-maker-part-two/><span class=title>Next »</span><br><span>Deploying Object Detection App into Android & EDGE Devices Part Two</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://www.aivision.app>AI Vision</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>